{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5afca213e3fd4a905a1ee04cf013c4a2a70ed6bf52820ba3c074a5841fff5c71"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predict (before training) 4 4.0\nEpoch: 0 w= 1.0933333333333333 loss= 4.666666666666667\nPredict (after training) 4 4.373333333333333\nEpoch: 1 w= 1.1779555555555554 loss= 3.8362074074074086\nPredict (after training) 4 4.711822222222222\nEpoch: 2 w= 1.2546797037037036 loss= 3.1535329869958857\nPredict (after training) 4 5.0187188148148145\nEpoch: 3 w= 1.3242429313580246 loss= 2.592344272332262\nPredict (after training) 4 5.2969717254320985\nEpoch: 4 w= 1.3873135910979424 loss= 2.1310222071581117\nPredict (after training) 4 5.54925436439177\nEpoch: 5 w= 1.4444976559288012 loss= 1.7517949663820642\nPredict (after training) 4 5.777990623715205\nEpoch: 6 w= 1.4963445413754464 loss= 1.440053319920117\nPredict (after training) 4 5.9853781655017855\nEpoch: 7 w= 1.5433523841804047 loss= 1.1837878313441108\nPredict (after training) 4 6.173409536721619\nEpoch: 8 w= 1.5859728283235668 loss= 0.9731262101573632\nPredict (after training) 4 6.343891313294267\nEpoch: 9 w= 1.6246153643467005 loss= 0.7999529948031382\nPredict (after training) 4 6.498461457386802\nEpoch: 10 w= 1.659651263674342 loss= 0.6575969151946154\nPredict (after training) 4 6.638605054697368\nEpoch: 11 w= 1.6914171457314033 loss= 0.5405738908195378\nPredict (after training) 4 6.765668582925613\nEpoch: 12 w= 1.7202182121298057 loss= 0.44437576375991855\nPredict (after training) 4 6.880872848519223\nEpoch: 13 w= 1.7463311789976905 loss= 0.365296627844598\nPredict (after training) 4 6.985324715990762\nEpoch: 14 w= 1.7700069356245727 loss= 0.3002900634939416\nPredict (after training) 4 7.080027742498291\nEpoch: 15 w= 1.7914729549662791 loss= 0.2468517784170642\nPredict (after training) 4 7.1658918198651165\nEpoch: 16 w= 1.8109354791694263 loss= 0.2029231330489788\nPredict (after training) 4 7.243741916677705\nEpoch: 17 w= 1.8285815011136133 loss= 0.16681183417217407\nPredict (after training) 4 7.314326004454453\nEpoch: 18 w= 1.8445805610096762 loss= 0.1371267415488235\nPredict (after training) 4 7.378322244038705\nEpoch: 19 w= 1.8590863753154396 loss= 0.11272427607497944\nPredict (after training) 4 7.4363455012617585\nEpoch: 20 w= 1.872238313619332 loss= 0.09266436490145864\nPredict (after training) 4 7.488953254477328\nEpoch: 21 w= 1.8841627376815275 loss= 0.07617422636521683\nPredict (after training) 4 7.53665095072611\nEpoch: 22 w= 1.8949742154979183 loss= 0.06261859959338009\nPredict (after training) 4 7.579896861991673\nEpoch: 23 w= 1.904776622051446 loss= 0.051475271914629306\nPredict (after training) 4 7.619106488205784\nEpoch: 24 w= 1.9136641373266443 loss= 0.04231496130368814\nPredict (after training) 4 7.654656549306577\nEpoch: 25 w= 1.9217221511761575 loss= 0.03478477885657844\nPredict (after training) 4 7.68688860470463\nEpoch: 26 w= 1.9290280837330496 loss= 0.02859463421027894\nPredict (after training) 4 7.716112334932198\nEpoch: 27 w= 1.9356521292512983 loss= 0.023506060193480772\nPredict (after training) 4 7.742608517005193\nEpoch: 28 w= 1.9416579305211772 loss= 0.01932302619282764\nPredict (after training) 4 7.766631722084709\nEpoch: 29 w= 1.9471031903392007 loss= 0.015884386331668398\nPredict (after training) 4 7.788412761356803\nEpoch: 30 w= 1.952040225907542 loss= 0.01305767153735723\nPredict (after training) 4 7.808160903630168\nEpoch: 31 w= 1.9565164714895047 loss= 0.010733986344664803\nPredict (after training) 4 7.826065885958019\nEpoch: 32 w= 1.9605749341504843 loss= 0.008823813841374291\nPredict (after training) 4 7.842299736601937\nEpoch: 33 w= 1.9642546069631057 loss= 0.007253567147113681\nPredict (after training) 4 7.857018427852423\nEpoch: 34 w= 1.9675908436465492 loss= 0.005962754575689583\nPredict (after training) 4 7.870363374586197\nEpoch: 35 w= 1.970615698239538 loss= 0.004901649272531298\nPredict (after training) 4 7.882462792958152\nEpoch: 36 w= 1.9733582330705144 loss= 0.004029373553099482\nPredict (after training) 4 7.893432932282058\nEpoch: 37 w= 1.975844797983933 loss= 0.0033123241439168096\nPredict (after training) 4 7.903379191935732\nEpoch: 38 w= 1.9780992835054327 loss= 0.0027228776607060357\nPredict (after training) 4 7.912397134021731\nEpoch: 39 w= 1.980143350378259 loss= 0.002238326453885249\nPredict (after training) 4 7.920573401513036\nEpoch: 40 w= 1.9819966376762883 loss= 0.001840003826269386\nPredict (after training) 4 7.927986550705153\nEpoch: 41 w= 1.983676951493168 loss= 0.0015125649231412608\nPredict (after training) 4 7.934707805972672\nEpoch: 42 w= 1.9852004360204722 loss= 0.0012433955919298103\nPredict (after training) 4 7.940801744081889\nEpoch: 43 w= 1.9865817286585614 loss= 0.0010221264385926248\nPredict (after training) 4 7.946326914634246\nEpoch: 44 w= 1.987834100650429 loss= 0.0008402333603648631\nPredict (after training) 4 7.951336402601716\nEpoch: 45 w= 1.9889695845897222 loss= 0.0006907091659248264\nPredict (after training) 4 7.955878338358889\nEpoch: 46 w= 1.9899990900280147 loss= 0.0005677936325753796\nPredict (after training) 4 7.959996360112059\nEpoch: 47 w= 1.9909325082920666 loss= 0.0004667516012495216\nPredict (after training) 4 7.963730033168266\nEpoch: 48 w= 1.9917788075181404 loss= 0.000383690560742734\nPredict (after training) 4 7.967115230072562\nEpoch: 49 w= 1.9925461188164473 loss= 0.00031541069384432885\nPredict (after training) 4 7.970184475265789\nEpoch: 50 w= 1.9932418143935788 loss= 0.0002592816085930997\nPredict (after training) 4 7.972967257574315\nEpoch: 51 w= 1.9938725783835114 loss= 0.0002131410058905752\nPredict (after training) 4 7.975490313534046\nEpoch: 52 w= 1.994444471067717 loss= 0.00017521137977565514\nPredict (after training) 4 7.977777884270868\nEpoch: 53 w= 1.9949629871013967 loss= 0.0001440315413480261\nPredict (after training) 4 7.979851948405587\nEpoch: 54 w= 1.9954331083052663 loss= 0.0001184003283899171\nPredict (after training) 4 7.981732433221065\nEpoch: 55 w= 1.9958593515301082 loss= 9.733033217332803e-05\nPredict (after training) 4 7.983437406120433\nEpoch: 56 w= 1.9962458120539648 loss= 8.000985883901657e-05\nPredict (after training) 4 7.984983248215859\nEpoch: 57 w= 1.9965962029289281 loss= 6.57716599593935e-05\nPredict (after training) 4 7.986384811715713\nEpoch: 58 w= 1.9969138906555615 loss= 5.406722767150764e-05\nPredict (after training) 4 7.987655562622246\nEpoch: 59 w= 1.997201927527709 loss= 4.444566413387458e-05\nPredict (after training) 4 7.988807710110836\nEpoch: 60 w= 1.9974630809584561 loss= 3.65363112808981e-05\nPredict (after training) 4 7.989852323833825\nEpoch: 61 w= 1.9976998600690001 loss= 3.0034471708953996e-05\nPredict (after training) 4 7.9907994402760005\nEpoch: 62 w= 1.9979145397958935 loss= 2.4689670610172655e-05\nPredict (after training) 4 7.991658159183574\nEpoch: 63 w= 1.9981091827482769 loss= 2.0296006560253656e-05\nPredict (after training) 4 7.9924367309931075\nEpoch: 64 w= 1.9982856590251044 loss= 1.6684219437262796e-05\nPredict (after training) 4 7.993142636100417\nEpoch: 65 w= 1.9984456641827613 loss= 1.3715169898293847e-05\nPredict (after training) 4 7.993782656731045\nEpoch: 66 w= 1.9985907355257035 loss= 1.1274479219506377e-05\nPredict (after training) 4 7.994362942102814\nEpoch: 67 w= 1.9987222668766378 loss= 9.268123006398985e-06\nPredict (after training) 4 7.994889067506551\nEpoch: 68 w= 1.9988415219681517 loss= 7.61880902783969e-06\nPredict (after training) 4 7.995366087872607\nEpoch: 69 w= 1.9989496465844576 loss= 6.262999634617916e-06\nPredict (after training) 4 7.9957985863378305\nEpoch: 70 w= 1.9990476795699081 loss= 5.1484640551938914e-06\nPredict (after training) 4 7.996190718279633\nEpoch: 71 w= 1.9991365628100501 loss= 4.232266273994499e-06\nPredict (after training) 4 7.996546251240201\nEpoch: 72 w= 1.999217150281112 loss= 3.479110977946351e-06\nPredict (after training) 4 7.996868601124448\nEpoch: 73 w= 1.999290216254875 loss= 2.859983851026929e-06\nPredict (after training) 4 7.9971608650195\nEpoch: 74 w= 1.9993564627377531 loss= 2.3510338359374262e-06\nPredict (after training) 4 7.9974258509510125\nEpoch: 75 w= 1.9994165262155628 loss= 1.932654303533636e-06\nPredict (after training) 4 7.997666104862251\nEpoch: 76 w= 1.999470983768777 loss= 1.5887277332523938e-06\nPredict (after training) 4 7.997883935075108\nEpoch: 77 w= 1.9995203586170245 loss= 1.3060048068548734e-06\nPredict (after training) 4 7.998081434468098\nEpoch: 78 w= 1.9995651251461022 loss= 1.0735939958924364e-06\nPredict (after training) 4 7.998260500584409\nEpoch: 79 w= 1.9996057134657994 loss= 8.825419799121559e-07\nPredict (after training) 4 7.998422853863198\nEpoch: 80 w= 1.9996425135423248 loss= 7.254887315754342e-07\nPredict (after training) 4 7.998570054169299\nEpoch: 81 w= 1.999675878945041 loss= 5.963839812987369e-07\nPredict (after training) 4 7.998703515780164\nEpoch: 82 w= 1.999706130243504 loss= 4.902541385825727e-07\nPredict (after training) 4 7.998824520974016\nEpoch: 83 w= 1.9997335580874436 loss= 4.0301069098738336e-07\nPredict (after training) 4 7.998934232349774\nEpoch: 84 w= 1.9997584259992822 loss= 3.312926995781724e-07\nPredict (after training) 4 7.999033703997129\nEpoch: 85 w= 1.9997809729060159 loss= 2.723373231729343e-07\nPredict (after training) 4 7.9991238916240635\nEpoch: 86 w= 1.9998014154347876 loss= 2.2387338352920307e-07\nPredict (after training) 4 7.9992056617391505\nEpoch: 87 w= 1.9998199499942075 loss= 1.8403387118941732e-07\nPredict (after training) 4 7.99927979997683\nEpoch: 88 w= 1.9998367546614149 loss= 1.5128402140063082e-07\nPredict (after training) 4 7.9993470186456594\nEpoch: 89 w= 1.9998519908930161 loss= 1.2436218932547864e-07\nPredict (after training) 4 7.9994079635720645\nEpoch: 90 w= 1.9998658050763347 loss= 1.0223124683409346e-07\nPredict (after training) 4 7.999463220305339\nEpoch: 91 w= 1.9998783299358769 loss= 8.403862850836479e-08\nPredict (after training) 4 7.9995133197435075\nEpoch: 92 w= 1.9998896858085284 loss= 6.908348768398496e-08\nPredict (after training) 4 7.999558743234114\nEpoch: 93 w= 1.9998999817997325 loss= 5.678969725349543e-08\nPredict (after training) 4 7.99959992719893\nEpoch: 94 w= 1.9999093168317574 loss= 4.66836551287917e-08\nPredict (after training) 4 7.99963726732703\nEpoch: 95 w= 1.9999177805941268 loss= 3.8376039345125727e-08\nPredict (after training) 4 7.999671122376507\nEpoch: 96 w= 1.9999254544053418 loss= 3.154680994333735e-08\nPredict (after training) 4 7.999701817621367\nEpoch: 97 w= 1.9999324119941766 loss= 2.593287985380858e-08\nPredict (after training) 4 7.999729647976706\nEpoch: 98 w= 1.9999387202080534 loss= 2.131797981222471e-08\nPredict (after training) 4 7.999754880832214\nEpoch: 99 w= 1.9999444396553017 loss= 1.752432687141379e-08\nPredict (after training) 4 7.999777758621207\n"
     ]
    }
   ],
   "source": [
    "# 一个大的batch\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = 1.0   #随便选一个初始值\n",
    "def forward(x):\n",
    "    return x * w\n",
    "def cost(xs, ys):\n",
    "    cost = 0\n",
    "    for x, y in zip(xs, ys): \n",
    "        y_pred = forward(x)\n",
    "        cost += (y_pred - y) ** 2\n",
    "    return cost / len(xs)\n",
    "def gradient(xs, ys):\n",
    "    grad = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        grad += 2 * x * (x * w - y)\n",
    "    return grad / len(xs)\n",
    "print('Predict (before training)', 4, forward(4)) \n",
    "for epoch in range(100):\n",
    "    cost_val = cost(x_data, y_data)\n",
    "    grad_val = gradient(x_data, y_data)\n",
    "    w -= 0.01 * grad_val\n",
    "    print('Epoch:', epoch, 'w=', w, 'loss=', cost_val)\n",
    "    print('Predict (after training)', 4, forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(1, 1, 0, 'z'), (2, 2, 1, 'h'), (3, 3, 2, 'a'), (4, 4, 3, 'n'), (5, 5, 4, 'g')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[1,2,3,4,5]\n",
    "b=(1,2,3,4,5)\n",
    "c=np.arange(5)\n",
    "d=\"zhang\"\n",
    "print(list(zip(a,b,c,d)))   #可以去掉list看看出来的是啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.], requires_grad=True)\ntensor([1.])\ntorch.FloatTensor\ntorch.FloatTensor\nNone\n<class 'NoneType'>\n1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "a = torch.tensor([1.0])\n",
    "a.requires_grad = True # 或者 a.requires_grad_()\n",
    "print(a)\n",
    "print(a.data)\n",
    "print(a.type())             # a的类型是tensor\n",
    "print(a.data.type())        # a.data的类型是tensor\n",
    "print(a.grad)\n",
    "print(type(a.grad))\n",
    "print(a.data.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "progress: 0 7.315943717956543\nw 1.260688066482544\nprogress: 1 3.9987640380859375\nw 1.4534177780151367\nprogress: 2 2.1856532096862793\nw 1.5959051847457886\nprogress: 3 1.1946394443511963\nw 1.7012479305267334\nprogress: 4 0.6529689431190491\nw 1.779128909111023\nprogress: 5 0.35690122842788696\nw 1.836707353591919\nprogress: 6 0.195076122879982\nw 1.8792757987976074\nprogress: 7 0.10662525147199631\nw 1.9107471704483032\nprogress: 8 0.0582793727517128\nw 1.9340143203735352\nprogress: 9 0.03185431286692619\nw 1.9512161016464233\nprogress: 10 0.017410902306437492\nw 1.9639335870742798\nprogress: 11 0.009516451507806778\nw 1.9733357429504395\nprogress: 12 0.005201528314501047\nw 1.9802868366241455\nprogress: 13 0.0028430151287466288\nw 1.9854258298873901\nprogress: 14 0.0015539465239271522\nw 1.989225149154663\nprogress: 15 0.0008493617060594261\nw 1.9920340776443481\nprogress: 16 0.00046424579340964556\nw 1.9941107034683228\nprogress: 17 0.0002537401160225272\nw 1.9956458806991577\nprogress: 18 0.00013869594840798527\nw 1.9967811107635498\nprogress: 19 7.580435340059921e-05\nw 1.9976202249526978\nprogress: 20 4.143271507928148e-05\nw 1.99824059009552\nprogress: 21 2.264650902361609e-05\nw 1.9986991882324219\nprogress: 22 1.2377059647405986e-05\nw 1.9990383386611938\nprogress: 23 6.768445018678904e-06\nw 1.9992889165878296\nprogress: 24 3.7000872907810844e-06\nw 1.999474287033081\nprogress: 25 2.021880391112063e-06\nw 1.9996113777160645\nprogress: 26 1.1044940038118511e-06\nw 1.9997127056121826\nprogress: 27 6.041091182851233e-07\nw 1.9997875690460205\nprogress: 28 3.296045179013163e-07\nw 1.9998430013656616\nprogress: 29 1.805076408345485e-07\nw 1.9998838901519775\nprogress: 30 9.874406714516226e-08\nw 1.9999140501022339\nprogress: 31 5.4147676564753056e-08\nw 1.9999364614486694\nprogress: 32 2.9467628337442875e-08\nw 1.999953031539917\nprogress: 33 1.6088051779661328e-08\nw 1.9999653100967407\nprogress: 34 8.734787115827203e-09\nw 1.9999743700027466\nprogress: 35 4.8466972657479346e-09\nw 1.9999810457229614\nprogress: 36 2.6520865503698587e-09\nw 1.999985933303833\nprogress: 37 1.4551915228366852e-09\nw 1.9999895095825195\nprogress: 38 7.914877642178908e-10\nw 1.9999922513961792\nprogress: 39 4.4019543565809727e-10\nw 1.9999942779541016\nprogress: 40 2.3283064365386963e-10\nw 1.9999958276748657\nprogress: 41 1.2028067430946976e-10\nw 1.9999970197677612\nprogress: 42 5.820766091346741e-11\nw 1.999997854232788\nprogress: 43 3.842615114990622e-11\nw 1.9999983310699463\nprogress: 44 2.2737367544323206e-11\nw 1.999998688697815\nprogress: 45 1.4551915228366852e-11\nw 1.9999990463256836\nprogress: 46 5.6843418860808015e-12\nw 1.9999992847442627\nprogress: 47 3.637978807091713e-12\nw 1.9999994039535522\nprogress: 48 3.637978807091713e-12\nw 1.9999995231628418\nprogress: 49 2.0463630789890885e-12\nw 1.9999996423721313\nprogress: 50 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 51 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 52 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 53 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 54 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 55 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 56 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 57 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 58 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 59 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 60 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 61 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 62 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 63 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 64 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 65 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 66 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 67 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 68 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 69 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 70 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 71 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 72 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 73 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 74 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 75 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 76 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 77 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 78 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 79 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 80 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 81 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 82 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 83 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 84 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 85 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 86 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 87 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 88 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 89 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 90 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 91 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 92 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 93 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 94 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 95 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 96 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 97 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 98 9.094947017729282e-13\nw 1.9999996423721313\nprogress: 99 9.094947017729282e-13\nw 1.9999996423721313\npredict (after training) 4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_data = [1.0, 2.0, 3.0] \n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = torch.Tensor([1.0]) \n",
    "w.requires_grad = True\n",
    "def forward(x):\n",
    "    return x * w    \n",
    "def loss(xs, ys):\n",
    "    y = forward(xs)\n",
    "    return (y-ys)**2\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_() \n",
    "    print(\"progress:\", epoch, l.item())\n",
    "    print(\"w\",w.data.item())\n",
    "print(\"predict (after training)\", 4, forward(4).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 loss: 43.04933547973633 a 0.23258423805236816 b 1.283128023147583 c 1.6771759986877441\n",
      "epoch 1 loss: 14.719114303588867 a 1.0675642490386963 b 1.5835927724838257 c 1.787113904953003\n",
      "epoch 2 loss: 1.0651649236679077 a 0.6911706924438477 b 1.4119752645492554 c 1.6972432136535645\n",
      "epoch 3 loss: 1.449332356452942 a 0.8779559135437012 b 1.4612475633621216 c 1.7018367052078247\n",
      "epoch 4 loss: 0.01829000748693943 a 0.8022456169128418 b 1.4089248180389404 c 1.6639125347137451\n",
      "epoch 5 loss: 0.3663938045501709 a 0.8482401967048645 b 1.4050121307373047 c 1.6471521854400635\n",
      "epoch 6 loss: 0.1299343705177307 a 0.8371891975402832 b 1.3796687126159668 c 1.6218717098236084\n",
      "epoch 7 loss: 0.20184975862503052 a 0.8522809147834778 b 1.3653489351272583 c 1.6018317937850952\n",
      "epoch 8 loss: 0.14731234312057495 a 0.8548237085342407 b 1.3469113111495972 c 1.580580234527588\n",
      "epoch 9 loss: 0.15293006598949432 a 0.8628348708152771 b 1.3313617706298828 c 1.5610716342926025\n",
      "epoch 10 loss: 0.1335027515888214 a 0.8679461479187012 b 1.3154033422470093 c 1.5418859720230103\n",
      "epoch 11 loss: 0.12665244936943054 a 0.8740647435188293 b 1.3005338907241821 c 1.5236400365829468\n",
      "epoch 12 loss: 0.11538296192884445 a 0.8793854117393494 b 1.2860214710235596 c 1.5060045719146729\n",
      "epoch 13 loss: 0.10718902200460434 a 0.884761393070221 b 1.2721734046936035 c 1.489092469215393\n",
      "epoch 14 loss: 0.09863202273845673 a 0.8898078799247742 b 1.2588146924972534 c 1.472812294960022\n",
      "epoch 15 loss: 0.0911700427532196 a 0.8947153091430664 b 1.2459957599639893 c 1.4571690559387207\n",
      "epoch 16 loss: 0.08408227562904358 a 0.8994066715240479 b 1.2336634397506714 c 1.4421244859695435\n",
      "epoch 17 loss: 0.07762365788221359 a 0.9039285778999329 b 1.2218140363693237 c 1.4276617765426636\n",
      "epoch 18 loss: 0.07161898165941238 a 0.9082694053649902 b 1.2104218006134033 c 1.4137555360794067\n",
      "epoch 19 loss: 0.06609360128641129 a 0.9124453067779541 b 1.1994726657867432 c 1.4003856182098389\n",
      "epoch 20 loss: 0.06098060682415962 a 0.9164576530456543 b 1.1889475584030151 c 1.3875305652618408\n",
      "epoch 21 loss: 0.056264642626047134 a 0.9203156232833862 b 1.1788312196731567 c 1.3751709461212158\n",
      "epoch 22 loss: 0.05190778151154518 a 0.9240238070487976 b 1.16910719871521 c 1.3632874488830566\n",
      "epoch 23 loss: 0.04788453131914139 a 0.9275881052017212 b 1.1597603559494019 c 1.3518617153167725\n",
      "epoch 24 loss: 0.04417134076356888 a 0.9310147762298584 b 1.1507765054702759 c 1.3408761024475098\n",
      "epoch 25 loss: 0.04074147343635559 a 0.9343084096908569 b 1.1421412229537964 c 1.3303133249282837\n",
      "epoch 26 loss: 0.037575434893369675 a 0.9374744892120361 b 1.1338412761688232 c 1.3201572895050049\n",
      "epoch 27 loss: 0.03465309366583824 a 0.9405180215835571 b 1.125863790512085 c 1.3103922605514526\n",
      "epoch 28 loss: 0.03195396438241005 a 0.943443238735199 b 1.1181960105895996 c 1.3010029792785645\n",
      "epoch 29 loss: 0.029463699087500572 a 0.9462554454803467 b 1.1108262538909912 c 1.2919751405715942\n",
      "epoch 30 loss: 0.027163652703166008 a 0.9489583373069763 b 1.1037427186965942 c 1.283294677734375\n",
      "epoch 31 loss: 0.025041507557034492 a 0.9515566229820251 b 1.09693443775177 c 1.2749481201171875\n",
      "epoch 32 loss: 0.023082101717591286 a 0.9540539383888245 b 1.090390920639038 c 1.2669225931167603\n",
      "epoch 33 loss: 0.021274501457810402 a 0.9564546942710876 b 1.084101915359497 c 1.2592058181762695\n",
      "epoch 34 loss: 0.01960543729364872 a 0.958761990070343 b 1.0780572891235352 c 1.2517856359481812\n",
      "epoch 35 loss: 0.018065765500068665 a 0.9609799981117249 b 1.0722479820251465 c 1.2446508407592773\n",
      "epoch 36 loss: 0.016644861549139023 a 0.9631119966506958 b 1.066664695739746 c 1.2377903461456299\n",
      "epoch 37 loss: 0.01533359382301569 a 0.965161144733429 b 1.0612987279891968 c 1.2311934232711792\n",
      "epoch 38 loss: 0.014123677276074886 a 0.967130720615387 b 1.05614173412323 c 1.2248499393463135\n",
      "epoch 39 loss: 0.013007381930947304 a 0.9690237045288086 b 1.0511854887008667 c 1.21875\n",
      "epoch 40 loss: 0.0119783328846097 a 0.9708438515663147 b 1.0464226007461548 c 1.2128844261169434\n",
      "epoch 41 loss: 0.01102770771831274 a 0.9725924134254456 b 1.0418449640274048 c 1.2072439193725586\n",
      "epoch 42 loss: 0.01015190128237009 a 0.9742735624313354 b 1.0374459028244019 c 1.2018200159072876\n",
      "epoch 43 loss: 0.0093438271433115 a 0.9758893251419067 b 1.033218502998352 c 1.1966042518615723\n",
      "epoch 44 loss: 0.008598418906331062 a 0.9774421453475952 b 1.0291558504104614 c 1.1915885210037231\n",
      "epoch 45 loss: 0.00791145209223032 a 0.9789349436759949 b 1.0252519845962524 c 1.1867653131484985\n",
      "epoch 46 loss: 0.007277630269527435 a 0.9803694486618042 b 1.0215004682540894 c 1.1821268796920776\n",
      "epoch 47 loss: 0.006693343631923199 a 0.9817482233047485 b 1.0178954601287842 c 1.177666187286377\n",
      "epoch 48 loss: 0.006154733709990978 a 0.9830732941627502 b 1.0144312381744385 c 1.173376441001892\n",
      "epoch 49 loss: 0.005658495239913464 a 0.9843471050262451 b 1.0111026763916016 c 1.1692510843276978\n",
      "epoch 50 loss: 0.0052007026970386505 a 0.985571026802063 b 1.0079041719436646 c 1.1652836799621582\n",
      "epoch 51 loss: 0.004778949078172445 a 0.9867472052574158 b 1.0048308372497559 c 1.1614681482315063\n",
      "epoch 52 loss: 0.004390685353428125 a 0.9878780841827393 b 1.0018779039382935 c 1.1577986478805542\n",
      "epoch 53 loss: 0.004032422322779894 a 0.9889645576477051 b 0.9990405440330505 c 1.1542693376541138\n",
      "epoch 54 loss: 0.003702624002471566 a 0.9900086522102356 b 0.9963144063949585 c 1.1508749723434448\n",
      "epoch 55 loss: 0.0033990032970905304 a 0.9910123944282532 b 0.9936953186988831 c 1.1476104259490967\n",
      "epoch 56 loss: 0.0031190128065645695 a 0.9919765591621399 b 0.9911788105964661 c 1.1444705724716187\n",
      "epoch 57 loss: 0.0028614525217562914 a 0.9929033517837524 b 0.9887611269950867 c 1.1414506435394287\n",
      "epoch 58 loss: 0.0026241661980748177 a 0.993793785572052 b 0.9864383935928345 c 1.1385458707809448\n",
      "epoch 59 loss: 0.002406028565019369 a 0.9946499466896057 b 0.9842071533203125 c 1.1357518434524536\n",
      "epoch 60 loss: 0.0022047823294997215 a 0.9954720735549927 b 0.9820635318756104 c 1.1330643892288208\n",
      "epoch 61 loss: 0.002020117361098528 a 0.9962626695632935 b 0.9800044894218445 c 1.130479335784912\n",
      "epoch 62 loss: 0.0018497572746127844 a 0.9970217347145081 b 0.9780263304710388 c 1.1279927492141724\n",
      "epoch 63 loss: 0.001693408703431487 a 0.9977515339851379 b 0.9761263132095337 c 1.125600814819336\n",
      "epoch 64 loss: 0.001549438457004726 a 0.9984526634216309 b 0.9743011593818665 c 1.1232998371124268\n",
      "epoch 65 loss: 0.001417099847458303 a 0.9991263151168823 b 0.9725481271743774 c 1.1210863590240479\n",
      "epoch 66 loss: 0.0012955375714227557 a 0.9997738599777222 b 0.970864474773407 c 1.1189569234848022\n",
      "epoch 67 loss: 0.0011836214689537883 a 1.0003955364227295 b 0.9692472815513611 c 1.116908311843872\n",
      "epoch 68 loss: 0.0010811459505930543 a 1.0009936094284058 b 0.9676944017410278 c 1.114937424659729\n",
      "epoch 69 loss: 0.0009866016916930676 a 1.0015674829483032 b 0.9662027955055237 c 1.1130412817001343\n",
      "epoch 70 loss: 0.000900041195563972 a 1.0021191835403442 b 0.964770495891571 c 1.1112170219421387\n",
      "epoch 71 loss: 0.0008205109043046832 a 1.0026490688323975 b 0.9633950591087341 c 1.109461784362793\n",
      "epoch 72 loss: 0.0007475242018699646 a 1.0031580924987793 b 0.9620742797851562 c 1.107772946357727\n",
      "epoch 73 loss: 0.0006806210149079561 a 1.0036473274230957 b 0.9608061909675598 c 1.1061480045318604\n",
      "epoch 74 loss: 0.0006192246219143271 a 1.0041172504425049 b 0.9595885872840881 c 1.1045843362808228\n",
      "epoch 75 loss: 0.0005629450897686183 a 1.0045686960220337 b 0.9584196209907532 c 1.1030796766281128\n",
      "epoch 76 loss: 0.0005113283987157047 a 1.0050021409988403 b 0.9572972655296326 c 1.101631760597229\n",
      "epoch 77 loss: 0.00046412250958383083 a 1.0054186582565308 b 0.9562198519706726 c 1.1002382040023804\n",
      "epoch 78 loss: 0.00042088335612788796 a 1.0058187246322632 b 0.9551857113838196 c 1.0988972187042236\n",
      "epoch 79 loss: 0.0003813207149505615 a 1.0062029361724854 b 0.9541929960250854 c 1.0976065397262573\n",
      "epoch 80 loss: 0.00034509089891798794 a 1.0065720081329346 b 0.9532402157783508 c 1.0963643789291382\n",
      "epoch 81 loss: 0.0003120153269264847 a 1.0069265365600586 b 0.9523257613182068 c 1.0951688289642334\n",
      "epoch 82 loss: 0.00028172507882118225 a 1.0072667598724365 b 0.9514480829238892 c 1.0940179824829102\n",
      "epoch 83 loss: 0.00025413522962480783 a 1.0075937509536743 b 0.9506059885025024 c 1.0929102897644043\n",
      "epoch 84 loss: 0.00022894705762155354 a 1.007907748222351 b 0.9497979283332825 c 1.0918439626693726\n",
      "epoch 85 loss: 0.0002059754478977993 a 1.0082093477249146 b 0.949022650718689 c 1.0908173322677612\n",
      "epoch 86 loss: 0.00018499544239602983 a 1.0084986686706543 b 0.9482787847518921 c 1.0898289680480957\n",
      "epoch 87 loss: 0.00016590277664363384 a 1.0087765455245972 b 0.9475651979446411 c 1.0888773202896118\n",
      "epoch 88 loss: 0.00014856956840958446 a 1.009043574333191 b 0.9468808174133301 c 1.0879610776901245\n",
      "epoch 89 loss: 0.00013274126104079187 a 1.0092995166778564 b 0.9462242722511292 c 1.0870786905288696\n",
      "epoch 90 loss: 0.00011844690016005188 a 1.0095455646514893 b 0.945594847202301 c 1.0862290859222412\n",
      "epoch 91 loss: 0.00010545575059950352 a 1.009781837463379 b 0.9449912309646606 c 1.0854108333587646\n",
      "epoch 92 loss: 9.362457785755396e-05 a 1.0100082159042358 b 0.9444122910499573 c 1.0846227407455444\n",
      "epoch 93 loss: 8.298293687403202e-05 a 1.0102260112762451 b 0.9438574314117432 c 1.0838637351989746\n",
      "epoch 94 loss: 7.330934749916196e-05 a 1.010434627532959 b 0.943325400352478 c 1.0831326246261597\n",
      "epoch 95 loss: 6.463314639404416e-05 a 1.010635256767273 b 0.9428156614303589 c 1.0824284553527832\n",
      "epoch 96 loss: 5.674719432136044e-05 a 1.0108274221420288 b 0.9423269629478455 c 1.0817499160766602\n",
      "epoch 97 loss: 4.968285793438554e-05 a 1.0110119581222534 b 0.94185870885849 c 1.0810962915420532\n",
      "epoch 98 loss: 4.331359377829358e-05 a 1.0111887454986572 b 0.9414100646972656 c 1.080466389656067\n",
      "epoch 99 loss: 3.764976645470597e-05 a 1.0113589763641357 b 0.9409804344177246 c 1.0798596143722534\n"
     ]
    }
   ],
   "source": [
    "## homework 我们写一个y=x^2+x+1的，一个两个节点的计算图,我们用y=ax^2+bx+c来拟合，a,b,c是需要计算梯度的\n",
    "import torch \n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "# 训练集合\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [3.0,7.0,13.0]\n",
    "a = torch.Tensor([2.0])\n",
    "b = torch.Tensor([2.0])\n",
    "c = torch.Tensor([2.0])\n",
    "a.requires_grad = True\n",
    "b.requires_grad = True\n",
    "c.requires_grad = True\n",
    "# 定义前向传播函数\n",
    "def forward(x):\n",
    "    y = a*x*x + b*x + c\n",
    "    return y\n",
    "def loss(xs,ys):\n",
    "    y_pred = forward(xs)\n",
    "    return (ys-y_pred)**2\n",
    "# 训练过程\n",
    "for epoch in range(epochs):\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        l = loss(x,y)\n",
    "        l.backward()\n",
    "        a.data = a.data -learning_rate * a.grad.data\n",
    "        b.data = b.data -learning_rate * b.grad.data\n",
    "        c.data = c.data -learning_rate * c.grad.data\n",
    "        a.grad.data.zero_() \n",
    "        b.grad.data.zero_() \n",
    "        c.grad.data.zero_() \n",
    "    print(\"epoch\", epoch, \"loss:\", l.item(), \"a\", a.data.item(), \"b\", b.data.item(), \"c\", c.data.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改写成向量的形式\n",
    "import torch \n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "# 训练集合\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [3.0,7.0,13.0]\n",
    "a = torch.Tensor([2.0])\n",
    "b = torch.Tensor([2.0])\n",
    "c = torch.Tensor([2.0])\n",
    "a.requires_grad = True\n",
    "b.requires_grad = True\n",
    "c.requires_grad = True\n",
    "# 定义前向传播函数\n",
    "def forward(x):\n",
    "    y = a*x*x + b*x + c\n",
    "    return y\n",
    "def loss(xs,ys):\n",
    "    y_pred = forward(xs)\n",
    "    return (ys-y_pred)**2\n",
    "# 训练过程\n",
    "for epoch in range(epochs):\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        l = loss(x,y)\n",
    "        l.backward()\n",
    "        a.data = a.data -learning_rate * a.grad.data\n",
    "        b.data = b.data -learning_rate * b.grad.data\n",
    "        c.data = c.data -learning_rate * c.grad.data\n",
    "        a.grad.data.zero_() \n",
    "        b.grad.data.zero_() \n",
    "        c.grad.data.zero_() \n",
    "    print(\"epoch\", epoch, \"loss:\", l.item(), \"a\", a.data.item(), \"b\", b.data.item(), \"c\", c.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "#第五章用一种更具扩展性的写法来写神经网络\n",
    "import torch\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]]) \n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n",
    "x_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 16.323488235473633\n1 1.996829628944397\n2 0.389644056558609\n3 0.20513498783111572\n4 0.17987234890460968\n5 0.17255279421806335\n6 0.1673685908317566\n7 0.1625480055809021\n8 0.15788967907428741\n9 0.1533677577972412\n10 0.14897528290748596\n11 0.1447087675333023\n12 0.14056460559368134\n13 0.13653895258903503\n14 0.1326284557580948\n15 0.12883004546165466\n16 0.12514051795005798\n17 0.12155647575855255\n18 0.11807529628276825\n19 0.11469364911317825\n20 0.11140885204076767\n21 0.1082182377576828\n22 0.10511907935142517\n23 0.10210853815078735\n24 0.09918436408042908\n25 0.09634371101856232\n26 0.09358447045087814\n27 0.09090422093868256\n28 0.08830098807811737\n29 0.08577194809913635\n30 0.08331549167633057\n31 0.08092951774597168\n32 0.07861165702342987\n33 0.07636026293039322\n34 0.07417342066764832\n35 0.07204911857843399\n36 0.06998582929372787\n37 0.06798148900270462\n38 0.06603450328111649\n39 0.06414332985877991\n40 0.062306277453899384\n41 0.06052188575267792\n42 0.058788638561964035\n43 0.0571049228310585\n44 0.05546955391764641\n45 0.053880929946899414\n46 0.05233781039714813\n47 0.05083897337317467\n48 0.04938298836350441\n49 0.04796859994530678\n50 0.04659478738903999\n51 0.045260485261678696\n52 0.043964218348264694\n53 0.04270511865615845\n54 0.04148209095001221\n55 0.040294088423252106\n56 0.039140090346336365\n57 0.03801903873682022\n58 0.036930352449417114\n59 0.03587275743484497\n60 0.03484530374407768\n61 0.0338473841547966\n62 0.032878000289201736\n63 0.03193635493516922\n64 0.031021766364574432\n65 0.030133379623293877\n66 0.02927030622959137\n67 0.02843199484050274\n68 0.0276178065687418\n69 0.026826761662960052\n70 0.02605852670967579\n71 0.025312284007668495\n72 0.024587348103523254\n73 0.0238831527531147\n74 0.02319912239909172\n75 0.022534720599651337\n76 0.021889381110668182\n77 0.021262530237436295\n78 0.02065359428524971\n79 0.020062066614627838\n80 0.019487502053380013\n81 0.01892939768731594\n82 0.018387315794825554\n83 0.01786063238978386\n84 0.017349181696772575\n85 0.01685229130089283\n86 0.01636965200304985\n87 0.0159008726477623\n88 0.015445470809936523\n89 0.015003080479800701\n90 0.014573484659194946\n91 0.014156035147607327\n92 0.013750692829489708\n93 0.013356849551200867\n94 0.01297433115541935\n95 0.012602714821696281\n96 0.012241850607097149\n97 0.011891250498592854\n98 0.011550642549991608\n99 0.011219902895390987\nw =  1.9299964904785156\nb =  0.1591343879699707\ny_pred =  7.879120349884033\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# prepare dataset\n",
    "# x,y是矩阵，3行1列 也就是说总共有3个数据，每个数据只有1个特征\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    " \n",
    "#design model using class\n",
    "\"\"\"\n",
    "our model class should be inherit from nn.Module, which is base class for all neural network modules.\n",
    "member methods __init__() and forward() have to be implemented\n",
    "class nn.linear contain two member Tensors: weight and bias\n",
    "class nn.Linear has implemented the magic method __call__(),which enable the instance of the class can\n",
    "be called just like a function.Normally the forward() will be called \n",
    "\"\"\"\n",
    "#定义model，需要补全forward函数\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # (1,1)是指输入x和输出y的特征维度，这里数据集中的x和y的特征都是1维的\n",
    "        # 该线性层需要学习的参数是w和b  获取w/b的方式分别是~linear.weight/linear.bias\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "# 实例化model\n",
    "model = LinearModel()\n",
    "\n",
    "# 都早优化器以及loss计算方式\n",
    "# construct loss and optimizer\n",
    "# criterion = torch.nn.MSELoss(size_average = False)\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.02) # model.parameters()自动完成参数的初始化操作\n",
    " \n",
    "#训练过程 \n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data) # forward:predict\n",
    "    loss = criterion(y_pred, y_data) # forward: loss\n",
    "    print(epoch, loss.item())\n",
    "    loss.backward() # backward: autograd，自动计算梯度\n",
    "    optimizer.step() # update 参数，即更新w和b的值\n",
    "    optimizer.zero_grad() # the grad computer by .backward() will be accumulated. so before backward, remember set the grad to zero\n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    " \n",
    "x_test = torch.tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}