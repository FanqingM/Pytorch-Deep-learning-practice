{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5afca213e3fd4a905a1ee04cf013c4a2a70ed6bf52820ba3c074a5841fff5c71"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture7:处理多维输入\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# prepare dataset\n",
    "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = torch.from_numpy(xy[:, :-1]) # 第一个‘：’是指读取所有行，第二个‘：’是指从第一列开始，最后一列不要\n",
    "y_data = torch.from_numpy(xy[:, [-1]]) # [-1] 最后得到的是个矩阵\n",
    " \n",
    "# design model using class\n",
    " \n",
    " \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6) # 输入数据x的特征是8维，x有8个特征\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid() # 将其看作是网络的一层，而不是简单的函数使用,这里不能用relu，因为会出现大雨1的，会跟下面用的BCEloss冲突的。\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x)) # y hat\n",
    "        return x\n",
    " \n",
    " \n",
    "model = Model()\n",
    " \n",
    "# construct loss and optimizer\n",
    "# criterion = torch.nn.BCELoss(size_average = True)\n",
    "criterion = torch.nn.BCELoss(reduction='mean')  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    " \n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(300):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(loss.item())\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    " \n",
    "    optimizer.step()\n",
    " \n",
    " \n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 2.4867076873779297\n1 2.3713269233703613\n2 2.2966604232788086\n3 2.2477617263793945\n4 2.2139859199523926\n5 2.1886723041534424\n6 2.167980670928955\n7 2.1498048305511475\n8 2.1330246925354004\n9 2.117053985595703\n10 2.1015896797180176\n11 2.086474895477295\n12 2.071629047393799\n13 2.057009696960449\n14 2.042593479156494\n15 2.0283689498901367\n16 2.0143275260925293\n17 2.000464916229248\n18 1.9867770671844482\n19 1.9732615947723389\n20 1.9599151611328125\n21 1.9467357397079468\n22 1.9337204694747925\n23 1.920867681503296\n24 1.9081752300262451\n25 1.895640254020691\n26 1.88326096534729\n27 1.8710349798202515\n28 1.8589603900909424\n29 1.8470351696014404\n30 1.8352570533752441\n31 1.8236234188079834\n32 1.8121330738067627\n33 1.8007839918136597\n34 1.789573311805725\n35 1.7784998416900635\n36 1.7675611972808838\n37 1.7567555904388428\n38 1.7460813522338867\n39 1.7355358600616455\n40 1.7251181602478027\n41 1.7148257493972778\n42 1.7046570777893066\n43 1.6946101188659668\n44 1.6846833229064941\n45 1.6748751401901245\n46 1.6651830673217773\n47 1.6556062698364258\n48 1.646142601966858\n49 1.6367905139923096\n50 1.6275482177734375\n51 1.6184141635894775\n52 1.609386920928955\n53 1.6004648208618164\n54 1.5916463136672974\n55 1.5829298496246338\n56 1.5743138790130615\n57 1.565796971321106\n58 1.557377815246582\n59 1.5490548610687256\n60 1.5408265590667725\n61 1.5326913595199585\n62 1.5246484279632568\n63 1.5166959762573242\n64 1.5088329315185547\n65 1.5010578632354736\n66 1.493369221687317\n67 1.4857666492462158\n68 1.4782476425170898\n69 1.4708117246627808\n70 1.4634575843811035\n71 1.456183910369873\n72 1.4489896297454834\n73 1.441873550415039\n74 1.434834361076355\n75 1.4278712272644043\n76 1.420982837677002\n77 1.414168119430542\n78 1.4074259996414185\n79 1.4007556438446045\n80 1.3941559791564941\n81 1.3876255750656128\n82 1.3811640739440918\n83 1.3747698068618774\n84 1.3684422969818115\n85 1.362180471420288\n86 1.355983018875122\n87 1.3498494625091553\n88 1.3437788486480713\n89 1.3377702236175537\n90 1.3318226337432861\n91 1.3259353637695312\n92 1.3201074600219727\n93 1.314338207244873\n94 1.3086262941360474\n95 1.302971363067627\n96 1.2973726987838745\n97 1.291829228401184\n98 1.286340355873108\n99 1.2809052467346191\nw =  0.8341392278671265\nb =  -2.0372259616851807\ny_pred =  tensor([[0.7857]])\n"
     ]
    }
   ],
   "source": [
    "# 第六讲 logistic回归\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    " \n",
    "# prepare dataset\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]]) # 3✖️1的tensor\n",
    "y_data = torch.Tensor([[0], [0], [1]])     # 同上\n",
    " \n",
    "#design model using class\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # y_pred = F.sigmoid(self.linear(x))\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "model = LogisticRegressionModel()\n",
    " \n",
    "# construct loss and optimizer\n",
    "# 这里所有样本就是一个batch进去训练的\n",
    "# 默认情况下，loss会基于element平均，如果size_average=False的话，loss会被累加。当我们采用mini-batch方法训练时，有时可能最后一个batch的batchSize与之前不一样就需要除一下。\n",
    "criterion = torch.nn.BCELoss(size_average = False) \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.07)\n",
    " \n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    " \n",
    "x_test = torch.Tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.294118"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "xy\n",
    "xy[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# prepare dataset\n",
    "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = torch.from_numpy(xy[:, :-1]) # 第一个‘：’是指读取所有行，第二个‘：’是指从第一列开始，最后一列不要\n",
    "print(\"input data.shape\", x_data.shape)\n",
    "y_data = torch.from_numpy(xy[:, [-1]]) # [-1] 最后得到的是个矩阵\n",
    " \n",
    "# print(x_data.shape)\n",
    "# design model using class\n",
    " \n",
    " \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 2)\n",
    "        self.linear4 = torch.nn.Linear(2, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x)) # y hat\n",
    "        x = self.linear4(x)  # y hat\n",
    "        return x\n",
    " \n",
    " \n",
    "model = Model()\n",
    " \n",
    "# construct loss and optimizer\n",
    "# criterion = torch.nn.BCELoss(size_average = True)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    " \n",
    " \n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # print(epoch, loss.item())\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    if epoch%10 == 9:\n",
    "        y_pred_label = torch.where(y_pred>=0.5,torch.tensor([1.0]),torch.tensor([0.0]))\n",
    " \n",
    "        acc = torch.eq(y_pred_label, y_data).sum().item()/y_data.size(0)\n",
    "        print(\"loss = \",loss.item(), \"acc = \",acc)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}